# Templatea Project Documentation

**Version:** 1.0.0
**Generated by:** Gemini
**Date:** 2025-10-09

## 1. Overview

Templatea is a comprehensive, API-driven video processing pipeline designed to automate the creation of templated video content from Instagram Reels. It fetches a reel, intelligently crops it, uses AI to perform Optical Character Recognition (OCR) and generate creative ad copy, and then renders a new video with custom branding and text overlays based on a flexible template system.

The system is exposed through a powerful FastAPI backend, providing REST endpoints and WebSocket support for real-time status tracking. It also includes an interactive Command-Line Interface (CLI) for manual, step-by-step processing.

### Key Features

- **End-to-End Automation**: From URL to final rendered video with minimal intervention.
- **AI-Powered Content Generation**: Utilizes Google Gemini for OCR and Groq for generating creative ad copy.
- **Intelligent Video Cropping**: Automatically detects and crops the main content from "video-in-video" formats.
- **Flexible Templating Engine**: Allows for multiple, distinct visual styles to be applied to rendered videos, defined by simple JSON manifests.
- **Robust Pipeline Orchestration**: An idempotent, file-based status system allows the pipeline to be resumed and re-run reliably.
- **Modern Web API**: A FastAPI application provides endpoints for triggering jobs, querying status, and retrieving video artifacts, with support for HTTP range requests for efficient video streaming.
- **Real-time Updates**: A WebSocket interface pushes live status updates as a video is processed through the pipeline.
- **Containerized Deployment**: Comes with Docker and Docker Compose configurations for easy, reproducible deployment.

## 2. Project Structure

The project is organized into several key directories, each with a distinct responsibility.

```
e:\Code\Templatea\
├── api/                    # Main FastAPI application and related modules.
│   ├── app.py              # Defines all HTTP and WebSocket endpoints.
│   ├── storage.py          # Manages the SQLite database and workspace file I/O.
│   ├── tasks.py            # Bridge between the API and the orchestrator script.
│   ├── template_registry.py# Discovers and manages rendering templates.
│   ├── db_init.py          # Initializes the database schema.
│   └── highlight_api.py    # Placeholder for AI text highlighting.
├── cli/                    # Command-Line Interface.
│   └── frontend_cli.py     # Interactive CLI for a step-by-step workflow.
├── db/                     # Database files.
│   └── workspaces.sqlite   # SQLite database for indexing workspaces.
├── templates/              # Template definition files.
│   ├── marketingspots.json # Manifest for the "marketingspots" template.
│   └── wishfulThinking.json# Manifest for the "wishfulThinking" template.
├── tests/                  # Pytest test suite.
│   ├── test_*.py           # Individual test files for different modules.
│   └── utils.py            # Test helper functions.
├── workspace/              # Output directory for all processed videos.
├── orchestrator.py         # Core script that manages the processing pipeline.
├── template_engine.py      # Main rendering engine using Pillow and ffmpeg.
├── instagram_downloader.py # Script to download Instagram reels.
├── video_detector.py       # Script for detecting and cropping inner videos.
├── gemini_ocr_batch.py     # Script for OCR and AI ad copy generation.
├── requirements.txt        # Python dependencies.
├── Dockerfile              # Docker build instructions.
└── docker-compose.yml      # Docker Compose service definition.
```

## 3. Getting Started

### 3.1. Prerequisites

-   Python 3.11+
-   `ffmpeg` installed and available in the system's PATH.
-   `yt-dlp` installed and available in the system's PATH.
-   API keys for AI services, set as environment variables:
    -   `GEMINI_API_KEY`: For Google Gemini (OCR).
    -   `GROQ_API_KEY`: For Groq (AI copy generation).
    -   `API_KEY`: A secret key to protect the API endpoints.

### 3.2. Local Installation

1.  **Create a virtual environment:**
    ```bash
    python -m venv .venv
    ```

2.  **Activate the environment:**
    -   Windows: `.\.venv\Scripts\activate`
    -   macOS/Linux: `source .venv/bin/activate`

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set environment variables** (example in PowerShell):
    ```powershell
    $env:API_KEY = "your-secret-api-key"
    $env:GEMINI_API_KEY = "your-gemini-api-key"
    $env:GROQ_API_KEY = "your-groq-api-key"
    ```

### 3.3. Running the Application

You can run the application in two ways:

1.  **Run the Web API (Recommended):**
    ```bash
    uvicorn api.app:APP --host 0.0.0.0 --port 8000
    ```
    The API will be available at `http://localhost:8000`.

2.  **Run the Interactive CLI:**
    ```bash
    python cli/frontend_cli.py
    ```
    The CLI will guide you through processing a single video.

### 3.4. Running with Docker

1.  **Build and run the Docker container:**
    ```bash
    docker-compose up --build
    ```
    This will build the image, start the container, and expose the API on port 8000. The `workspace/` and `db/` directories are mounted as volumes to persist data.

## 4. Architecture and Pipeline

The application is built around a 5-step, idempotent pipeline managed by the **Orchestrator**. Each step reads from the output of the previous one and writes its own status to a `.status` file in the workspace directory.

### 4.1. Workspace Structure

For each video, a unique "workspace" directory is created. All artifacts for that video are stored within this directory.

```
workspace/
└── <video_id>/
    ├── 00_raw/             # Downloaded video, metadata, and thumbnail.
    ├── 01_detector/        # Cropped video.
    ├── 02_ocr/             # Extracted text and AI-generated copy.
    ├── 03_choice/          # The final text chosen for rendering.
    ├── 04_render/          # The final rendered video.
    ├── logs/               # Logs for each step.
    ├── meta.json           # Metadata about the video and processing.
    └── *.status            # Status files for each pipeline step.
```

### 4.2. Pipeline Flow

1.  **Step 0: Download (`instagram_downloader.py`)**
    -   Takes an Instagram Reel URL.
    -   Creates a new workspace.
    -   Downloads the video, caption, and thumbnail using `instaloader` or `yt-dlp`.
    -   Normalizes filenames and creates the initial `meta.json`.

2.  **Step 1: Detect (`video_detector.py`)**
    -   Analyzes the downloaded video to find an "inner video" (e.g., a screen recording).
    -   Uses computer vision techniques (temporal variance, SSIM) to identify the dynamic region.
    -   Crops the video to this region and saves it. If no inner video is detected, it copies the original.

3.  **Step 2: OCR & AI Copy (`gemini_ocr_batch.py`)**
    -   Extracts a frame from the (cropped) video.
    -   Sends the frame to the **Google Gemini API** to perform OCR and extract text.
    -   Sends the extracted text (and the original downloader caption, if available) to the **Groq API**.
    -   Groq generates three alternative, "punchy" one-liner ad copy suggestions.
    -   Saves the OCR text and AI copy suggestions to the workspace.

4.  **Step 3: Choice**
    -   This step waits for a final text to be chosen for rendering.
    -   The choice can be made automatically (`--auto` flag in the orchestrator) or manually via the API or CLI.
    -   The chosen text is written to `03_choice/choice.txt`.

5.  **Step 4: Render (`template_engine.py`)**
    -   Reads the chosen text and the cropped video.
    -   Loads the specified template (from `meta.json`).
    -   Uses `Pillow` to create a canvas with the background, text, and logo.
    -   Uses `ffmpeg` to composite the canvas over the video, creating the final `output.mp4`.

### 4.3. API Layer (`api/`)

The FastAPI application in the `api/` directory provides a clean, modern interface to the pipeline.

-   **`app.py`**: The main entry point. It defines all endpoints, including:
    -   `POST /api/v1/reels`: To submit a new URL for processing.
    -   `GET /api/v1/workspaces`: To list all processed videos.
    -   `GET /api/v1/workspaces/{id}`: To get detailed information about a single video.
    -   `GET /api/v1/workspaces/{id}/files/{key}`: To download any artifact (video, log, text). Supports HTTP range requests.
    -   `POST /api/v1/workspaces/{id}/choice`: To submit the chosen text for rendering.
    -   `WS /ws/workspace/{id}`: A WebSocket endpoint for receiving live status updates.
-   **`storage.py`**: A crucial module that abstracts away all file system and database interactions. It uses a SQLite database to index workspaces for fast querying and manages the reading and writing of `meta.json` and `.status` files.
-   **`tasks.py`**: The link between the web world and the script world. When a new job is submitted via the API, this module runs `orchestrator.py` as a background process. It also contains the `WorkspaceEventBus`, a simple in-memory pub/sub system to forward events from the orchestrator to the WebSocket.

## 5. Core Modules Breakdown

-   **`orchestrator.py`**: The heart of the system. It's designed to be idempotent, meaning it can be run multiple times on the same workspace without causing issues. It checks the `.status` files to see which steps have already been completed and only runs the necessary ones. It uses file locks (`.lock`) to prevent multiple orchestrator processes from working on the same workspace simultaneously.

-   **`template_engine.py`**: A highly flexible rendering engine. It reads a `template.json` file to get configuration for canvas size, background, font, text position, and logo. It dynamically adjusts the font size to ensure the text fits within the specified constraints. It uses `ffmpeg` for the final, high-quality video composition.

-   **`gemini_ocr_batch.py`**: A powerful script that encapsulates the AI functionality. It calls the Gemini API for OCR and then uses a sophisticated prompt to instruct the Groq API to act as an expert copywriter, returning a clean JSON object with ad copy suggestions. It includes fallbacks and error handling to ensure the pipeline doesn't break if an AI service fails.

-   **`video_detector.py`**: This script demonstrates advanced computer vision techniques. By analyzing the variance between frames over time, it can distinguish a static background from a dynamic inner video. It then refines the bounding box of the inner video using edge detection and structural similarity (SSIM) to ensure a clean crop.

## 6. How to Use

### 6.1. Using the API

1.  **Start the API server:** `uvicorn api.app:APP --port 8000`
2.  **Submit a URL for processing:**
    ```bash
    curl -X POST "http://localhost:8000/api/v1/reels" \
         -H "Content-Type: application/json" \
         -H "X-API-Key: your-secret-api-key" \
         -d '{ 
               "url": "https://www.instagram.com/reel/Cxyz...",
               "template_id": "marketingspots",
               "auto": true
             }'
    ```
3.  **List workspaces:**
    ```bash
    curl http://localhost:8000/api/v1/workspaces
    ```
4.  **View a workspace and its files:**
    ```bash
    curl http://localhost:8000/api/v1/workspaces/<workspace_id>
    ```
5.  **Connect to the WebSocket** to receive live updates for a workspace:
    `ws://localhost:8000/ws/workspace/<workspace_id>`

### 6.2. Using the CLI

1.  **Run the interactive CLI:**
    ```bash
    python cli/frontend_cli.py
    ```
2.  Follow the prompts to enter a URL, select a template, and choose the final text. The CLI will run the orchestrator and wait for each step to complete, providing you with real-time feedback.

## 7. Template System

The template system is designed to be easily extensible.

-   **Manifest (`templates/*.json`)**: Each template is defined by a JSON file in the `templates/` directory. This file contains the template's `id`, `name`, and, most importantly, the `module` and `entrypoint` that point to the Python rendering function.
-   **Registry (`api/template_registry.py`)**: The template registry automatically discovers these JSON files at startup. When a render is requested, it dynamically imports the specified module and gets the renderer function.
-   **Renderer Function**: A renderer function (like `render_with_template` in `template_engine.py`) is a Python function that accepts the input video path, output path, text, and an optional configuration dictionary. This design allows you to create new Python scripts with different rendering logic and easily integrate them by just adding a new JSON manifest.

```